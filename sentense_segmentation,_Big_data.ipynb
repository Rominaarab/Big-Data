{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzQJWsnUqOIZ",
        "outputId": "a6c5a9e8-4a92-4e30-cac3-15dc06ce73b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abNuUSF5qXLu",
        "outputId": "c4723557-a93b-43cd-b8ca-c763dd740142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "RRshTh5uqj_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "nCXrEk0crEy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"Big data is transforming industries across the globe. It allows companies to gain insights from massive datasets. This leads to better decision-making and innovation.\"\"\""
      ],
      "metadata": {
        "id": "5vqn6CkprKbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(paragraph)"
      ],
      "metadata": {
        "id": "JRdaUgNSrPIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sentences = [sent.text for sent in doc.sents]"
      ],
      "metadata": {
        "id": "AJgphzc1rSe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "  print(f\"Sentence {i}: {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrFS-pcErU1z",
        "outputId": "9cd398b2-5526-4416-e163-af0559296564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: Big data is transforming industries across the globe.\n",
            "Sentence 2: It allows companies to gain insights from massive datasets.\n",
            "Sentence 3: This leads to better decision-making and innovation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sent in enumerate(doc.sents, 1):\n",
        "    print(f\"\\nSentence {i}: {sent.text.strip()}\")\n",
        "    print(\"Tokens:\")\n",
        "    for token in sent:\n",
        "        print(f\"  - {token.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0N4LGmrfrZpS",
        "outputId": "e0b8ca5d-5cac-4de5-e1a2-1815e4f2ad06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence 1: Big data is transforming industries across the globe.\n",
            "Tokens:\n",
            "  - Big\n",
            "  - data\n",
            "  - is\n",
            "  - transforming\n",
            "  - industries\n",
            "  - across\n",
            "  - the\n",
            "  - globe\n",
            "  - .\n",
            "\n",
            "Sentence 2: It allows companies to gain insights from massive datasets.\n",
            "Tokens:\n",
            "  - It\n",
            "  - allows\n",
            "  - companies\n",
            "  - to\n",
            "  - gain\n",
            "  - insights\n",
            "  - from\n",
            "  - massive\n",
            "  - datasets\n",
            "  - .\n",
            "\n",
            "Sentence 3: This leads to better decision-making and innovation.\n",
            "Tokens:\n",
            "  - This\n",
            "  - leads\n",
            "  - to\n",
            "  - better\n",
            "  - decision\n",
            "  - -\n",
            "  - making\n",
            "  - and\n",
            "  - innovation\n",
            "  - .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4MpnO9bzxx3",
        "outputId": "e59abe0c-6c89-44d7-8753-33785c844579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "3lDUGmJf8Rqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTK9TizM--wB",
        "outputId": "fd680fee-9e34-46d9-943d-49094a3cd5ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "uu5vW6ty-uJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sent in enumerate(sentences, 1):\n",
        "    print(f\"\\nSentence {i}: {sent.strip()}\")\n",
        "    print(\"Tokens:\")\n",
        "    # Tokenize the sentence into words and punctuation\n",
        "    tokens = nltk.word_tokenize(sent)\n",
        "    for token in tokens:\n",
        "        print(f\"  - {token}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igeQJ4mq-zMN",
        "outputId": "d8d5487e-4e4f-4281-f15d-e347ba83c76a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence 1: Big data is transforming industries across the globe.\n",
            "Tokens:\n",
            "  - Big\n",
            "  - data\n",
            "  - is\n",
            "  - transforming\n",
            "  - industries\n",
            "  - across\n",
            "  - the\n",
            "  - globe\n",
            "  - .\n",
            "\n",
            "Sentence 2: It allows companies to gain insights from massive datasets.\n",
            "Tokens:\n",
            "  - It\n",
            "  - allows\n",
            "  - companies\n",
            "  - to\n",
            "  - gain\n",
            "  - insights\n",
            "  - from\n",
            "  - massive\n",
            "  - datasets\n",
            "  - .\n",
            "\n",
            "Sentence 3: This leads to better decision-making and innovation.\n",
            "Tokens:\n",
            "  - This\n",
            "  - leads\n",
            "  - to\n",
            "  - better\n",
            "  - decision-making\n",
            "  - and\n",
            "  - innovation\n",
            "  - .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "XA5Q4D4S_U6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def character_tokenize_with_word_tokenize(text, use_word_preprocess=True):\n",
        "    \"\"\"\n",
        "    Perform character-level tokenization of a given text with optional preprocessing using word_tokenize from NLTK.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input string to tokenize into characters.\n",
        "        use_word_preprocess (bool): If True, preprocesses the text using word_tokenize before splitting into characters.\n",
        "                                    Default is True to utilize the imported word_tokenize.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of individual characters from the input text.\n",
        "    \"\"\"\n",
        "    # Preprocess using word_tokenize if requested\n",
        "    if use_word_preprocess:\n",
        "        words = word_tokenize(text)\n",
        "        # Rejoin the tokenized words into a single string (preserves NLTK's handling of punctuation)\n",
        "        text = ' '.join(words)\n",
        "\n",
        "    # Perform character-level tokenization using Python\n",
        "    char_tokens = list(text)\n",
        "    return char_tokens\n",
        "\n",
        "# Example usage\n",
        "text = \"Big data is amazing!\"\n",
        "char_tokens = character_tokenize_with_word_tokenize(text, use_word_preprocess=True)\n",
        "\n",
        "# Print the result\n",
        "print(\"Input Text:\", text)\n",
        "print(\"Character Tokens:\")\n",
        "for i, char in enumerate(char_tokens, 1):\n",
        "    print(f\"  - Token {i}: '{char}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzHCSZ0RBXY4",
        "outputId": "6a2ed487-ba47-41c6-aba1-72ebcaa4a23e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text: Big data is amazing!\n",
            "Character Tokens:\n",
            "  - Token 1: 'B'\n",
            "  - Token 2: 'i'\n",
            "  - Token 3: 'g'\n",
            "  - Token 4: ' '\n",
            "  - Token 5: 'd'\n",
            "  - Token 6: 'a'\n",
            "  - Token 7: 't'\n",
            "  - Token 8: 'a'\n",
            "  - Token 9: ' '\n",
            "  - Token 10: 'i'\n",
            "  - Token 11: 's'\n",
            "  - Token 12: ' '\n",
            "  - Token 13: 'a'\n",
            "  - Token 14: 'm'\n",
            "  - Token 15: 'a'\n",
            "  - Token 16: 'z'\n",
            "  - Token 17: 'i'\n",
            "  - Token 18: 'n'\n",
            "  - Token 19: 'g'\n",
            "  - Token 20: ' '\n",
            "  - Token 21: '!'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5DpSHVVHKaN",
        "outputId": "a8a9f6ca-5f5b-4f17-e6fb-820ba8933372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers) (0.31.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "import os"
      ],
      "metadata": {
        "id": "Kh9vwlMSG1R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_bpe_tokenizer(corpus_file, vocab_size=5000, output_path=\"bpe_tokenizer\"):\n",
        "    \"\"\"\n",
        "    Build and train a Byte Pair Encoding (BPE) tokenizer using Hugging Face tokenizers library.\n",
        "\n",
        "    Args:\n",
        "        corpus_file (str): Path to the text file used for training the tokenizer.\n",
        "        vocab_size (int): The size of the vocabulary to build (number of merges).\n",
        "        output_path (str): Directory to save the trained tokenizer.\n",
        "\n",
        "    Returns:\n",
        "        Tokenizer: Trained BPE tokenizer.\n",
        "    \"\"\"\n",
        "    # Initialize a tokenizer with BPE model\n",
        "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "\n",
        "    # Set a pre-tokenizer to split on whitespace\n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "    # Define the trainer with desired vocabulary size and special tokens\n",
        "    trainer = BpeTrainer(\n",
        "        vocab_size=vocab_size,\n",
        "        show_progress=True,\n",
        "        special_tokens=[\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "    )\n",
        "\n",
        "    # Train the tokenizer on the provided corpus\n",
        "    tokenizer.train(files=[corpus_file], trainer=trainer)\n",
        "\n",
        "    # Save the tokenizer\n",
        "    if not os.path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "    tokenizer.save(f\"{output_path}/tokenizer.json\")\n",
        "\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "OAA2ppIjG3WK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Step 1: Prepare a sample corpus (replace with your own dataset)\n",
        "    sample_text = \"\"\"\n",
        "    Big data refers to extremely large data sets that may be analyzed computationally to reveal patterns,\n",
        "    trends, and associations, especially relating to human behavior and interactions. It includes data streams,\n",
        "    static archives, structured and unstructured data, and comes from many sources like social media, sensors,\n",
        "    and transaction logs. Handling big data at scale requires specialized tools and architectures.\n",
        "    \"\"\"\n",
        "\n",
        "    # Save sample text to a temporary file for training\n",
        "    corpus_file = \"sample_corpus.txt\"\n",
        "    with open(corpus_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(sample_text)\n",
        "\n",
        "    # Step 2: Build and train the BPE tokenizer\n",
        "    vocab_size = 100  # Small vocab size for demo; increase for real applications\n",
        "    tokenizer = build_bpe_tokenizer(corpus_file, vocab_size=vocab_size, output_path=\"bpe_tokenizer\")\n",
        "\n",
        "    # Step 3: Demonstrate tokenization\n",
        "    test_text = \"Big data is amazing for analyzing trends!\"\n",
        "    encoded = tokenizer.encode(test_text)\n",
        "\n",
        "    print(\"\\nOriginal Text:\", test_text)\n",
        "    print(\"Token IDs:\", encoded.ids)\n",
        "    print(\"Tokens:\", encoded.tokens)\n",
        "\n",
        "    # Step 4: Demonstrate decoding\n",
        "    decoded_text = tokenizer.decode(encoded.ids)\n",
        "    print(\"Decoded Text:\", decoded_text)\n",
        "\n",
        "    # Clean up temporary file\n",
        "    if os.path.exists(corpus_file):\n",
        "        os.remove(corpus_file)"
      ],
      "metadata": {
        "id": "_Cn_h4zqG-Nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ByiXeynGToo",
        "outputId": "53e30db8-9ed7-4707-af3f-a71bcb4e38f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original Text: Big data is amazing for analyzing trends!\n",
            "Token IDs: [78, 44, 18, 27, 81, 10, 33, 72, 15, 23, 26, 35, 37, 32, 33, 72, 68, 22, 89, 0]\n",
            "Tokens: ['Big', 'data', 'i', 's', 'am', 'a', 'z', 'ing', 'f', 'o', 'r', 'an', 'al', 'y', 'z', 'ing', 'tre', 'n', 'ds', '[UNK]']\n",
            "Decoded Text: Big data i s am a z ing f o r an al y z ing tre n ds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KmnDz4K1JxUK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}